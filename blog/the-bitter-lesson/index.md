---
slug: the-bitter-lesson
title: The Bitter Lesson (And Why Your Clever AI Trick is Probably Doomed)
authors: [flabat]
tags: [LLMs, compute, scaling, humor, AI]
---

Alright, gather 'round, fellow AI enthusiasts, data scientists, and anyone who's ever proudly declared, "My algorithm is *so* much smarter than brute force!" Today, we're talking about something called "The Bitter Lesson," coined by the venerable Rich Sutton. And let me tell you, it's less of a gentle nudge and more of a cosmic wedgie for anyone who thinks their handcrafted AI feature is the key to unlocking artificial general intelligence.

<!-- truncate -->

Picture this: You spend months, maybe years, meticulously crafting the perfect knowledge representation for your AI. You build intricate symbolic logic systems, design elegant ontologies, and fine-tune heuristics based on deep domain expertise. You feel like Dr. Frankenstein, but, you know, with less grave-robbing and more Python libraries. Your creation is beautiful, nuanced, and understands the problem *so well*.

Then, along comes Chad. Chad didn't spend years understanding the nuances. Chad found a bigger server rack, threw a metric ton of data at a relatively simple, general-purpose learning algorithm (think deep learning before it was cool, or maybe just *more* deep learning now), cranked up the compute, and went for a long lunch.

Guess whose AI performs better? Yeah. It's Chad's. Every. Single. Time.

That, my friends, is the Bitter Lesson in a nutshell. Sutton observed that over decades of AI research, the approaches that ultimately win out aren't the ones based on clever human insights or intricate domain knowledge baked into the algorithm. Nope. The winners are the ones that leverage **general methods** combined with **massive amounts of computation**. Scale beats smarts. Brute force, fueled by Moore's Law (or whatever cloud provider's invoice you can stomach), eventually crushes elegant, human-designed complexity.

It's bitter because it feels like our intelligence, our creativity, our *understanding* should count for more. We want to believe that crafting the perfect algorithm is like composing a symphony. The Bitter Lesson suggests it's more like realizing the loudest orchestra wins, regardless of whether they're playing Mozart or just banging pots and pans really, *really* hard, but with *billions* of pots and pans.

Think about it. Chess? Deep Blue didn't "understand" chess like Kasparov. It calculated bazillions of moves. Go? AlphaGo used deep learning and massive search, not ancient Go proverbs. Language translation? Gigantic neural networks trained on the entire internet, not painstakingly curated grammar rules. Image recognition? More layers, more data, more GPUs!

So, what's a clever AI researcher to do? Give up and become a professional GPU wrangler? Well, maybe not *entirely*. Human ingenuity is still crucial for designing those general methods, figuring out *how* to scale them, and defining the problems in the first place. But the lesson suggests we should focus less on encoding our *own* knowledge directly into the system and more on creating systems that can *learn* that knowledge (and much more) from vast amounts of data and computation.

It's humbling. It's frustrating. It makes you want to throw your carefully tuned hyperparameters out the window. But hey, at least Chad seems happy with his giant server rack. Maybe he'll let us borrow some compute time... if we ask nicely? Or maybe we just need to find an even *bigger* pile of pots and pans.
